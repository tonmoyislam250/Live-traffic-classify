{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jieoi/traffic_sign_recognition/blob/updating/7_model_training_RRDB_enhanced_data_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4bbpXqXE-ys"
      },
      "source": [
        "Note: This instance of Google Colab has been configured to use T4 GPU. It is possible to run all other sections at a slower rate on CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGeJSyPpFI_t"
      },
      "source": [
        "Note: The code below has been configured to run on Google colab with python scripts and data imported from google drive. Please download the scripts and data from [here](https://drive.google.com/drive/folders/1nViTcpc952b0ftRGqkDBFqbSEC01t0a8). Files required including all scripts and helper files under data_preparation folder and data_augmentation folder. Alternatively, vist my github repository to [download](https://github.com/Jieoi/traffic_sign_recognition/tree/main) the scripts and helper files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX68P2z6KqgN"
      },
      "source": [
        "<h1> 7. Machine Learning on ESR-GAN Enhanced Images using CNN</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L2NqqJpQDnr"
      },
      "source": [
        "Based on Notebook 2 [Training data preparation using RRDB](https://github.com/Jieoi/traffic_sign_recognition/blob/main/2_train_data_preparation_RRDB.ipynb), this notebook is used to train a CNN model on the enhanced training data.\n",
        "\n",
        "The testing data is also loaded to evaluate the models as they are being developed.\n",
        "\n",
        "The testing data is from Notebook 4 [Testing data preparation using simple technic](https://github.com/Jieoi/traffic_sign_recognition/blob/main/4_test_data_preparation_RRDB.ipynb).\n",
        "\n",
        "There is **no need to run the notebooks named above**, a copy of the prepared data is saved in the Google drive which will be loaded in 7.1.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA8lAuWTQ4Bd"
      },
      "source": [
        "<h2> 7.1 Data Preparation for Machine Learning</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqaFF_bla7G4"
      },
      "source": [
        "Firstly, the data that were enhanced and augmented were loaded from the Google drive. As this colab instance is configured to use T4 GPU, some configuration checks are conducted to ensure the GPU is available. Libraries that are needed for machine learning were also imported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5uxRK7kKx_U"
      },
      "source": [
        "<h2>7.1 Preparing the notebook</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COiJJylxK30z"
      },
      "source": [
        "The first step is to connect to Google Drive so that the data enhanced in the previouse notebook can be used.\n",
        "\n",
        "The exact files that the notebook needs to  access are <code>image_enhanced_PIL_RRDB.zip</code> and <code>image_dataRRDB.csv</code>.\n",
        "They can be found in the train_data and test_data folders <a href=\"https://drive.google.com/drive/folders/1nViTcpc952b0ftRGqkDBFqbSEC01t0a8\">here</a>.\n",
        "\n",
        "\n",
        "Ideally, the notebook should be run in Google Colab with high RAM and GPU support. However, the code can be excuted on CPU only mode with a longer training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKBTqdfmQUr1",
        "outputId": "09329212-f89f-42d5-c3b5-9a02d5496a48"
      },
      "outputs": [],
      "source": [
        "# Checking GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0mxftvRqAu"
      },
      "source": [
        "The output shows that the GPU is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SzA6v6VRsiI"
      },
      "source": [
        "Google drive is also mounted to this colab instance. Data is then extracted from the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hnGCcs_4sNy",
        "outputId": "a1c672e6-c869-4864-cf62-d9c9b90f05d5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBNIWx1Km-Q"
      },
      "source": [
        "The libraries used in this notebook are listed below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEemnXizTEw4"
      },
      "outputs": [],
      "source": [
        "# Zip file handling\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Data handling and visualization\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File handling and management for training and validation data\n",
        "from shutil import copyfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# importing library for ML processing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# CNN layers\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Regularization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Saving the model\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Metrics for evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ycXAdhSGV7"
      },
      "source": [
        "<h2>7.1.2 Data preparation for Machine Learning</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znQglMe3mU_p"
      },
      "source": [
        "Firstly, pre-porcessed training data is obtained and prepared for machine learning:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKBDwP7kNu-L"
      },
      "source": [
        "Then the images are extracted from the <code>image_enhanced_PIL_RRDB.zip</code> file. It is first extracted in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aegfPAhacJRR",
        "outputId": "231157d6-c98c-44f9-9997-a0b3c5c7303e"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "zip_file_path = 'drive/MyDrive/final/train_data/image_enhanced_PIL_RRDB.zip'\n",
        "extracted_dir = 'extracted_images'\n",
        "\n",
        "# Create the directory for extracted images if it doesn't exist\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Open the ZIP file and extract its contents\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "# Check the length of the extracted images\n",
        "num_extracted_images = len(os.listdir(extracted_dir))\n",
        "print(f\"Number of extracted images: {num_extracted_images}\")\n",
        "print(\"Extraction completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RWYzZpNM_LA"
      },
      "source": [
        "As the labels were saved in the file name in the image files during enhancement and augmentation, the class labels are extracted from file name and printed below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DcZpKxb5bREV",
        "outputId": "176c68b3-8eb9-4d66-bfc1-5ea44fae1894"
      },
      "outputs": [],
      "source": [
        "# Define the directory where the images are extracted\n",
        "extracted_dir = 'extracted_images'\n",
        "\n",
        "# Get a list of all extracted image files\n",
        "image_files = os.listdir(extracted_dir)\n",
        "\n",
        "# Initialize lists to store image names and label numbers\n",
        "image_names = []\n",
        "label_numbers = []\n",
        "\n",
        "# Loop through the image files and extract image names and label numbers\n",
        "for image_file in image_files:\n",
        "    if image_file.endswith('.png'):\n",
        "        # Split the image file name to extract label number\n",
        "        parts = image_file.split('_')\n",
        "        if len(parts) >= 3:\n",
        "            label_number = int(parts[-1].split('.')[0])  # Extract the label number\n",
        "            image_names.append(image_file)\n",
        "            label_numbers.append(label_number)\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "data = {'filename': image_names, 'class': label_numbers}\n",
        "df_classes = pd.DataFrame(data)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "df_classes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgAfoUUON3vh"
      },
      "source": [
        "Both images and there labels extracted contains the same number of entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrskEW6ON0Zc"
      },
      "source": [
        "The distribution of the final classes is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "s28B6914YyMd",
        "outputId": "4355db06-8a9f-487a-a12c-1800563ae823"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each class in the DataFrame\n",
        "class_counts = df_classes['class'].value_counts()\n",
        "\n",
        "# Sort the unique class IDs\n",
        "unique_classes = np.sort(df_classes['class'].unique())\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(unique_classes, class_counts[unique_classes])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution')\n",
        "plt.xticks(unique_classes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K32jFqlSvyZ"
      },
      "source": [
        "This shows that the images were equally distributed among the classes as what has being prepared in the previous notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obTCtr8vORqc"
      },
      "source": [
        "Both images are first split into training and validation sets. They are grouped by their classes and stored in individual directories for every classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "husZ2lw6dYyC",
        "outputId": "b59a042a-336b-407a-fb0d-11db9d64754a"
      },
      "outputs": [],
      "source": [
        "train_dir = 'train_images'\n",
        "valid_dir = 'valid_images'\n",
        "\n",
        "# Create directories for training and validation images\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(valid_dir, exist_ok=True)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, valid_df = train_test_split(df_classes, test_size=0.2, random_state=42, stratify=df_classes['class'])\n",
        "\n",
        "# Copy images to directories based on classes for training set\n",
        "for index, row in train_df.iterrows():\n",
        "    src_path = os.path.join(extracted_dir, row['filename'])\n",
        "    dst_path = os.path.join(train_dir, str(row['class']), row['filename'])\n",
        "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "    copyfile(src_path, dst_path)\n",
        "\n",
        "# Copy images to directories based on classes for validation set\n",
        "for index, row in valid_df.iterrows():\n",
        "    src_path = os.path.join(extracted_dir, row['filename'])\n",
        "    dst_path = os.path.join(valid_dir, str(row['class']), row['filename'])\n",
        "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "    copyfile(src_path, dst_path)\n",
        "\n",
        "print(\"Data preparation for ML completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F61JPGGOoN2"
      },
      "source": [
        "<h1> 7.2 Machine Learning</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kumuDOWeSTrX"
      },
      "source": [
        "<h2>7.2.1 Data pipeline</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEkNgAgcRWjj"
      },
      "source": [
        "<code>flow_from_directory()</code> from Keras ImageDataGenerator is used to load the images with rescaling to 1/255. batch size and image size are pre-set to 32 and (128, 128), respectively. These number were based on the upscaling of images in image enhancement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICEfnqPEeJBl",
        "outputId": "ee2c7544-8435-40be-fb83-c9e9a013d8ff"
      },
      "outputs": [],
      "source": [
        "# Define image dimensions and batch size\n",
        "image_size = (128,128)\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators and rescale image for ML on training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale',  # Load images as grayscale\n",
        "    shuffle=True  # Set shuffle to True for training data\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale',  # Load images as grayscale\n",
        "    shuffle=False  # Set shuffle to False for validation data\n",
        ")\n",
        "\n",
        "# Get the number of classes from the training generator\n",
        "num_classes = len(train_generator.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQOkwn8gTVQ1"
      },
      "source": [
        "Total <b>21328</b> images are loaded into the training data and <b>5332</b> images are loaded into the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFiJylAImhVR"
      },
      "source": [
        "Next, the **test data** is also obtained and prepared for machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9NplueZTkgs"
      },
      "source": [
        "Firstly, the class labels are extracted from the <code>test_image_enhanced_PIL_RRDB.zip</code> file. As in the preparation of the testing data, the class labels were saved directly into the filenames of each images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_EztXSRduQJ"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract the class label from the image file name\n",
        "def extract_label(filename):\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) >= 3:\n",
        "        return parts[-2]  # Extract the label number\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjQBx8gQmnaV"
      },
      "outputs": [],
      "source": [
        "# Define the paths\n",
        "test_zip_path = 'drive/MyDrive/final/test_data/test_images_enhanced_PIL_RRDB.zip'\n",
        "test_extracted_dir = 'extracted_test_images'\n",
        "test_dir = 'test_images'\n",
        "\n",
        "# Create the directory for extracted test images if it doesn't exist\n",
        "os.makedirs(test_extracted_dir, exist_ok=True)\n",
        "\n",
        "# Open the ZIP file and extract its contents\n",
        "with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(test_extracted_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7txY676psmN"
      },
      "outputs": [],
      "source": [
        "# List all files in the extracted directory\n",
        "test_image_files = os.listdir(test_extracted_dir)\n",
        "\n",
        "# Create subdirectories based on class labels and move images\n",
        "for filename in test_image_files:\n",
        "    if filename.endswith('.png'):\n",
        "        label = extract_label(filename)\n",
        "        if label is not None:\n",
        "            class_dir = os.path.join(test_dir, label)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "            src_path = os.path.join(test_extracted_dir, filename)\n",
        "            dst_path = os.path.join(class_dir, filename)\n",
        "            copyfile(src_path, dst_path) # shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDsD5qw2KxTY"
      },
      "source": [
        "Data generator is used for loading images into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oelcW3EtnlSz",
        "outputId": "a1c78088-96df-48a5-bf6f-7395a7c542cb"
      },
      "outputs": [],
      "source": [
        "# Create a data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale',  # Load images as grayscale\n",
        "    shuffle=False  # Set shuffle to False for testing data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJUQK66CaB8W"
      },
      "source": [
        "<h2>7.2.2 Training the CNN model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsMRTT06SepR"
      },
      "source": [
        "Double checking the GPU availability and set the GPU device if it is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42FI4zL2feiP",
        "outputId": "1e297a9b-4d23-4f67-d86c-c6229671a24b"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Specify GPU device\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSCR24GDSyqc"
      },
      "source": [
        "A function is defined to plot the training history and calculate the changes in accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_5cFB1DjMZ4"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy_increase_rate(history):\n",
        "    train_accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    train_rate = np.mean(np.diff(train_accuracy))\n",
        "    val_rate = np.mean(np.diff(val_accuracy))\n",
        "\n",
        "    return train_rate, val_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZcwYOoBef3J"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    # Access the training history\n",
        "    training_loss = history.history['loss']\n",
        "    training_accuracy = history.history['accuracy']\n",
        "    validation_loss = history.history['val_loss']\n",
        "    validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plotting the training and validation accuracy\n",
        "    ax1.plot(training_accuracy)\n",
        "    ax1.plot(validation_accuracy)\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plotting the training and validation loss\n",
        "    ax2.plot(training_loss)\n",
        "    ax2.plot(validation_loss)\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    # Adjust the spacing between subplots\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwCQ055BKlkL"
      },
      "source": [
        "Then, true labels are extracted from the test generator for testing later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra0sBmAbv8QP"
      },
      "outputs": [],
      "source": [
        "# Get true labels from the test generator\n",
        "true_labels = test_generator.classes\n",
        "\n",
        "# Get unique true labels\n",
        "class_names = [str(label) for label in range(43)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSxiIAxXgrcr"
      },
      "source": [
        "The model development will start from developing a simple model that serves as a baseline in this development cycle. It is different from the basline of the whole project as this is trained on enhanced and augmented data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUFRqk4uAFq"
      },
      "source": [
        "**<h2> Model 1: Simple CNN baseline</h2>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6QVb1pmLGfH"
      },
      "source": [
        "As there are many models being developed for this project, a final check is conducted on the input images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJfJ3fBOhMQR",
        "outputId": "920f579f-30f7-4202-d3e1-f32676a5e02b"
      },
      "outputs": [],
      "source": [
        "# Get the first batch\n",
        "images, labels = next(train_generator)\n",
        "\n",
        "# Check the shape of the first image in the batch\n",
        "shape_of_first_image = images[0].shape\n",
        "\n",
        "# Print the shape\n",
        "print(\"Shape of the first training image:\", shape_of_first_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwyIjLpS3Em"
      },
      "source": [
        "Training the CNN model to upto 30 epochs, early stopping is used to prevent overfitting. However, no regularization is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2FXPHyGS1vm"
      },
      "outputs": [],
      "source": [
        "# Define the CNN layers\n",
        "input_layer = Input(shape=(128, 128, 1))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model_CNN = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model_CNN.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOpcPZc5LZvd"
      },
      "source": [
        "A simple summary of the model is displayed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1A1_tc-UtXG",
        "outputId": "e0e4369d-18a4-4acf-c11d-7d9bf585038f"
      },
      "outputs": [],
      "source": [
        "model_CNN.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6eVNsWmLeb0"
      },
      "source": [
        "The model is trained with the early stopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0sYx0qRTFsH",
        "outputId": "25899645-75c4-4862-d5b8-6b476e79c67e"
      },
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 30\n",
        "history_simpleCNN = model_CNN.fit(train_generator,\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=valid_generator,\n",
        "                                  callbacks=[early_stopping])\n",
        "\n",
        "print(\"Model training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yes-UARTLn4Q"
      },
      "source": [
        "The training stopped at <b>epoch 17</b> as the model started overfitting (shown by the validation loss). The training accuracy and validation accuracy graph are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Zn9XUczieoVY",
        "outputId": "219febf6-bb00-456e-e0f2-63ea53ec3913"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_simpleCNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Q4mfT5MAbo"
      },
      "source": [
        "This model has a lower training accuracy than validation accuracy. And the validaton accuracy is continuesly increasing, indicating no signs of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubZZ4EK_jRRD",
        "outputId": "c8e256ef-6f77-492f-97cc-a2f57cbc16f8"
      },
      "outputs": [],
      "source": [
        "# Increasing rate\n",
        "train_rate_SCNN, val_rate_SCNN = calculate_accuracy_increase_rate(history_simpleCNN)\n",
        "print(\"Training Accuracy Increase Rate:\", train_rate_SCNN)\n",
        "print(\"Validation Accuracy Increase Rate:\", val_rate_SCNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0zcqUUMZaV"
      },
      "source": [
        "The rate of increase for accuracy is calculated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6xrbzAMfe8"
      },
      "source": [
        "Then, the model is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU0BSx5cqIvH",
        "outputId": "6a4ac758-378d-4c7d-f68b-6c5fe3459885"
      },
      "outputs": [],
      "source": [
        "# Test the model with the test data\n",
        "test_loss_SCNN, test_accuracy_SCNN = model_CNN.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Loss: {test_loss_SCNN:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_SCNN:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRYMAq09qSus",
        "outputId": "013ceda5-5201-478c-8109-ac715d29629b"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "test_pred_SCNN = model_CNN.predict(test_generator)\n",
        "\n",
        "# Convert one-hot encoded predictions back to class labels\n",
        "pred_labels_SCNN = np.argmax(test_pred_SCNN, axis=1)\n",
        "\n",
        "# Calculate the precision score\n",
        "precision_SCNN = precision_score(true_labels, pred_labels_SCNN, average='weighted')\n",
        "\n",
        "print(f\"Precision Score: {precision_SCNN:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DxGUG22tefV",
        "outputId": "991a271c-06fb-422b-b38d-b94bc281c49e"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report\n",
        "class_rep_SCNN = classification_report(true_labels, pred_labels_SCNN, target_names=class_names)\n",
        "\n",
        "print(\"Classification Report:\\n\", class_rep_SCNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BlgERW0MqYM"
      },
      "source": [
        "Generally, the model is performing well with a test precision of 88.94% and test accuracy of 88.28%. For acciracy across all classes, the worst performing one is class 13 with 62% precision and 69% F1 score. It is also noticed that some classes have perfect scores on precision.\n",
        "\n",
        "Comparing across different models, this simple CNN model is performing better than the baseline model on CNN and better than the [simple CNN model with simple data enhancement](https://github.com/Jieoi/traffic_sign_recognition/blob/main/5_model_training_simple_enhanced_data_CNN.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2jCS4-t0Vj"
      },
      "source": [
        "**<h2> Model 2: More complex CNN with regularization</h2>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjUFO2mmT4RJ"
      },
      "source": [
        "More complexity is added to the CNN model with regularization added:\n",
        "\n",
        "*   Added more convolutional layers with increasing filters to capture more complex features\n",
        "\n",
        "*   Added batch normalization layers after each convolutional and dense layer to help stabilize training. (Recommended in chapter 7.3.1 Advanced architechtures in Deep Learning with Python, Francois Chollet)\n",
        "\n",
        "*   Added an additional dense layer with dropout and batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B57Siq1yYQZL"
      },
      "outputs": [],
      "source": [
        "# Define the CNN layers\n",
        "input_layer = Input(shape=(128, 128, 1))\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model_complex = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model_complex.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZUposRqN6U8"
      },
      "source": [
        "A summary of the model is displayed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXOBt8CkYt9-",
        "outputId": "58d40adc-05c7-4bd3-ed2a-3cd059239f9c"
      },
      "outputs": [],
      "source": [
        "model_complex.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xLR4-6OCRL"
      },
      "source": [
        "The model is trained with the early stopping callback monitering the validation loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YiIxl2kYbXX",
        "outputId": "1a3bee8c-3d08-4c1f-d074-da63034ba295"
      },
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with data augmentation and early stopping\n",
        "num_epochs = 50\n",
        "history_complex_model = model_complex.fit(train_generator,\n",
        "                                          epochs=num_epochs,\n",
        "                                          validation_data=valid_generator,\n",
        "                                          callbacks=[early_stopping])\n",
        "\n",
        "print(\"Model training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0fzCKSiONyM"
      },
      "source": [
        "The training stopped at <b>epoch 16</b> as the model started overfitting (shown by the validation loss). This happend <b>1 epoch</b> before the previous model. The training accuracy and validation accuracy graph are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "7LMYWjxCv3Ga",
        "outputId": "021af362-46a3-4b72-c974-d013b9a46ffa"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_complex_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZQHZFL4OXhE"
      },
      "source": [
        "This model has a lower training accuracy than validation accuracy until about <b>epoch 10</b>. And the validaton accuracy is continuesly increasing, indicating no signs of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHVWoQyrwTk6",
        "outputId": "194dae42-a009-4fc1-e8a4-1a0054c6799a"
      },
      "outputs": [],
      "source": [
        "# Increasing rate\n",
        "train_rate_complex, val_rate_complex = calculate_accuracy_increase_rate(history_complex_model)\n",
        "print(\"Training Accuracy Increase Rate:\", train_rate_complex)\n",
        "print(\"Validation Accuracy Increase Rate:\", val_rate_complex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGOsMfCkOwmq"
      },
      "source": [
        "The rate of increase for accuracy is calculated above. It is increasing at a **slower rate** than the simple CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8_sXeFZO4g-"
      },
      "source": [
        "Then, the model is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2UhqO-Wv3r4",
        "outputId": "b7120304-a22b-428c-8ffd-9ecce794b579"
      },
      "outputs": [],
      "source": [
        "# Test the model with the test data\n",
        "test_loss_complex, test_accuracy_complex = model_complex.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Loss: {test_loss_complex:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_complex:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGjoJfhUv31a",
        "outputId": "bf5affe1-0ab0-4f5f-97c2-99885afbcb08"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "test_pred_complex = model_complex.predict(test_generator)\n",
        "\n",
        "# Convert one-hot encoded predictions back to class labels\n",
        "predicted_labels_complex = np.argmax(test_pred_complex, axis=1)\n",
        "\n",
        "# Calculate the precision score\n",
        "precision_complex = precision_score(true_labels, predicted_labels_complex, average='weighted')\n",
        "\n",
        "print(f\"Precision Score: {precision_complex:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZr-SxJwJEQ",
        "outputId": "1a646a24-044f-42e3-c474-d5ba5d77a5dd"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report\n",
        "class_rep_complex = classification_report(true_labels, predicted_labels_complex, target_names=class_names)\n",
        "\n",
        "print(\"Classification Report:\\n\", class_rep_complex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-tuJpcsPDKH"
      },
      "source": [
        "Generally, the model is performing well with a test precision of 90.07% and test accuracy of 91.23%. For accuracy across all classes, the worst performing one is class 31 with 71% precision and 71% F1 score. It is also noticed that a few classes have perfect scores on precision.\n",
        "\n",
        "This model improved the performance of the simple CNN model especially on the accuracy across all classes on test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxP02q59wPQ2"
      },
      "source": [
        "**<h2> Model 3: Even more complex CNN with learning rate adjustment</h2>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-BrhCiR2Mp"
      },
      "source": [
        "An attempt has been made to add more layers to the model with a **lower learning rate**. It could potentially allow a **more complexed model** with less likelihood of overfitting, at a cost of **increased training time**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7yCUqraioFY"
      },
      "outputs": [],
      "source": [
        "# Define the CNN layers\n",
        "input_layer = Input(shape=(128, 128, 1))\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "# Create the model\n",
        "model_more_complex = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model_more_complex.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "                           loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtwjmB0vkwK6",
        "outputId": "54713ba9-0417-47d7-aea5-4a71e79474ed"
      },
      "outputs": [],
      "source": [
        "model_more_complex.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjvOOI-fjmqk",
        "outputId": "4f7f6402-5889-4389-f6f5-f94ef1c4b4fe"
      },
      "outputs": [],
      "source": [
        "# Compile the model with a lower learning rate\n",
        "model_more_complex.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "history_more_complex_model = model_more_complex.fit(train_generator, epochs=num_epochs, validation_data=valid_generator, callbacks=[early_stopping])\n",
        "\n",
        "print(\"Model training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "850KMf35Tk_0"
      },
      "source": [
        "The training stopped at **epoch 24** as the model started overfitting (shown by the validation loss). This happend 8 epoch after the previous model. The training accuracy and validation accuracy graph are shown below. It was noted that the **training accuracy** and the **validation accuracy** are both **very high**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "1ZIG9-9wVd4C",
        "outputId": "0cc82c3b-fb4f-4ed0-a273-b8f78fd8ae16"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_more_complex_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuRj8rxMTw7O"
      },
      "source": [
        "Generally, this model has a higher training accuracy than validation accuracy. Validaton accuracy experienced significant fluctuations since epoch 10. However, it is still no indications of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlPptPKHjZOQ",
        "outputId": "cb266bac-ac89-4d2f-c2da-d4d8070a85ff"
      },
      "outputs": [],
      "source": [
        "# Increasing rate\n",
        "train_rate_mComplex, val_rate_mComplex = calculate_accuracy_increase_rate(history_more_complex_model)\n",
        "print(\"Training Accuracy Increase Rate:\", train_rate_mComplex)\n",
        "print(\"Validation Accuracy Increase Rate:\", val_rate_mComplex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGC9BC-9UADe"
      },
      "source": [
        "The rate of increase for accuracy is calculated above. It is training at a much slower rate than the simple CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxy4LVUC7d"
      },
      "source": [
        "Then, the model is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOQJcz2duiua",
        "outputId": "9446d940-b4be-46fe-8136-89e6892964f1"
      },
      "outputs": [],
      "source": [
        "# Test the model with the test data\n",
        "test_loss_mComplex, test_accuracy_mComplex = model_more_complex.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Loss: {test_loss_mComplex:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_mComplex:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6PFojTuqq9",
        "outputId": "85dc8f4b-6447-456e-a013-368de9ecc323"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "test_pred_mComplex = model_more_complex.predict(test_generator)\n",
        "\n",
        "# Convert one-hot encoded predictions back to class labels\n",
        "predicted_labels_mComplex = np.argmax(test_pred_mComplex, axis=1)\n",
        "\n",
        "# Calculate the precision score\n",
        "precision_mComplex = precision_score(true_labels, predicted_labels_mComplex, average='weighted')\n",
        "\n",
        "print(f\"Precision Score: {precision_mComplex:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omduGPdyO7Nz",
        "outputId": "26c1613f-43f0-4f95-f89d-c044483bc77f"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report\n",
        "class_rep_mComplex = classification_report(true_labels, predicted_labels_mComplex, target_names=class_names)\n",
        "\n",
        "print(\"Classification Report:\\n\", class_rep_mComplex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8OIVC5jUIdj"
      },
      "source": [
        "Although the model is performing much better in training data, the performance on test data **very similar** to the previous model. **Althernative model is recommended** as this is learning at a slower rate with more computational power needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOkTitWY6R32"
      },
      "source": [
        "**<h2> Model 4: Even more complex CNN with regularization and more layers</h2>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXJEOPK-UY5A"
      },
      "source": [
        "As the previous model not performing well, an attempt has been made to add more layers to the model so that it could potentially allow a **more complexed model** with less likelihood of overfitting, at a cost of **increased training time**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7zUnmfG4p-a"
      },
      "outputs": [],
      "source": [
        "# Define a more complex CNN architecture\n",
        "input_layer = Input(shape=(128, 128, 1))\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add another dense layer with dropout\n",
        "x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model_modified = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model_modified.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAa6QXFUrX8"
      },
      "source": [
        "The above model is revised from all previous models. It added more omore set of dense layers and dropout layers with regularization. It also had a lower learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaUAF2Uv6jUu",
        "outputId": "a631e2f5-c86c-446a-fb24-abeffd253a1e"
      },
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "history_modified_model = model_modified.fit(train_generator,\n",
        "                                            epochs=num_epochs,\n",
        "                                            validation_data=valid_generator,\n",
        "                                            callbacks=[early_stopping])\n",
        "\n",
        "print(\"Model training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Ur4nKrVA3v"
      },
      "source": [
        "The training stopped at **epoch 31** as the model started overfitting (shown by the validation loss). This happend 7 epoch after the previous model. The training accuracy and validation accuracy graph are shown below. It was noted that the **training accuracy** and the **validation accuracy** are both **still very high**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "fqa7qLr162ml",
        "outputId": "f3195db4-02af-4d89-a32b-37762b5b2ddf"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_modified_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmPjS9B7VNck"
      },
      "source": [
        "This model has a similar training accuracy than validation accuracy. UnLike the previous model, validation accuracy did **not experienced significant fluctuations**. The model is thus much more **stable**. Moreover, it is still no indications of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJB695Q9Vm4D"
      },
      "source": [
        "  The model is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEcFmexjCGai",
        "outputId": "6358a1a8-ce5c-42e1-fcc9-335f960b27ae"
      },
      "outputs": [],
      "source": [
        "# Test the model with the test data\n",
        "test_loss_mComplex, test_accuracy_mComplex = model_modified.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Loss: {test_loss_mComplex:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_mComplex:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89FHIk2NCLAJ",
        "outputId": "5c35fa91-38be-497e-ded8-2c56a1144dc1"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "test_pred_modified = model_modified.predict(test_generator)\n",
        "\n",
        "# Convert one-hot encoded predictions back to class labels\n",
        "predicted_labels_modified = np.argmax(test_pred_modified, axis=1)\n",
        "\n",
        "# Calculate the precision score\n",
        "precision_modified = precision_score(true_labels, predicted_labels_modified, average='weighted')\n",
        "\n",
        "print(f\"Precision Score: {precision_modified:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxT6GtoTVtlJ"
      },
      "source": [
        "After this adjustment, the new model did **not perform better** than the previous model. Thus, the exploration of model structure is concluded.\n",
        "\n",
        "The project will more on to saving this model and training of other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPIoN9AEBbSc"
      },
      "source": [
        "<h1> 7.3 Saving the model</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKCIK4zNWESi"
      },
      "source": [
        "The raw predictions, training history, and model are saved below. They were all saved to google drive for reusing and analysis in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfuln5m4C3R7",
        "outputId": "a3757836-1f3a-4be9-a3b2-cf9a749b23c3"
      },
      "outputs": [],
      "source": [
        "# Define the file path where you want to save the raw predictions\n",
        "save_path = \"best_test_predictions_CNN_RRDB.npy\"\n",
        "\n",
        "# Save the raw predictions to the specified file\n",
        "np.save(save_path, test_pred_modified)\n",
        "\n",
        "print(f\"Raw predictions saved to '{save_path}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LFkXK6hBctD"
      },
      "source": [
        "Keras model is saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGhaZ8tGQC7A"
      },
      "outputs": [],
      "source": [
        "model_modified.save(\"final_CNN_model_RRDB.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms7QnaMcBf4d"
      },
      "source": [
        "saving training history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD8a_DTcBd5C"
      },
      "outputs": [],
      "source": [
        "def save_training_history(history, filename):\n",
        "    # Create a DataFrame from the training history\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    history_df.to_csv(filename, index=False)\n",
        "\n",
        "    # Print the first few rows of the history DataFrame\n",
        "    print(history_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIWdpGeiBwBV",
        "outputId": "1cae5eda-1427-4a09-b0e0-0eba43599311"
      },
      "outputs": [],
      "source": [
        "save_training_history(history_simpleCNN, 'training_history_simpleCNN_RRDB.csv')\n",
        "save_training_history(history_modified_model, 'training_history_complexCNN_RRDB.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKvK2ot4Bkdy"
      },
      "source": [
        "Moving to google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INCKfh3KDhys",
        "outputId": "af9f7ec5-93e2-4d9e-b191-3a6bc55fb63d"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the destination directory in Google Drive\n",
        "dest_dir = 'drive/MyDrive/final/training_models/CNN/RRDB/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Define the source paths\n",
        "src_paths = [\n",
        "    'final_CNN_model_RRDB.keras',\n",
        "    'training_history_simpleCNN_RRDB.csv',\n",
        "    'training_history_complexCNN_RRDB.csv',\n",
        "    'best_test_predictions_CNN_RRDB.npy'\n",
        "]\n",
        "\n",
        "# Copy files to Google Drive\n",
        "for src_path in src_paths:\n",
        "    shutil.copy(src_path, dest_dir)\n",
        "\n",
        "print(\"Files copied to Google Drive successfully.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO7ZsMtz8/lu/tnbx52s6lf",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
